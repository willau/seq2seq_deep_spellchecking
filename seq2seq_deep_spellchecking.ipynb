{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "from numpy.random import rand, randint, choice, shuffle\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# parameters\n",
    "FILEPATH = \"./data/news.2011.en.shuffled\"\n",
    "CLEAN_FILEPATH = \"./data/news.2011.en.cleaned\"\n",
    "N_LINES = 2466169\n",
    "CHARS = list(\" abcdefghijklmnopqrstuvwxyz\")\n",
    "MAX_INPUT_LEN = 40\n",
    "MIN_INPUT_LEN = 3\n",
    "AMOUNT_OF_NOISE = 0.3 / MAX_INPUT_LEN\n",
    "\n",
    "# regex cleanup\n",
    "RE_DASH_FILTER = re.compile(r'[\\-\\˗\\֊\\‐\\‑\\‒\\–\\—\\⁻\\₋\\−\\﹣\\－]', re.UNICODE)\n",
    "NORMALIZE_WHITESPACE_REGEX = re.compile(r'[^\\S\\n]+', re.UNICODE)\n",
    "RE_APOSTROPHE_FILTER = re.compile(r'&#39;|[ʼ՚＇‘’‛❛❜ߴߵ`‵´ˊˋ{}{}{}{}{}{}{}{}{}]'.\n",
    "                                  format ( \n",
    "                                      chr(768), chr(769), chr(832), \n",
    "                                      chr(833), chr(2387), chr(5151), \n",
    "                                      chr(5152), chr(65344), chr(8242)\n",
    "                                  ), re.UNICODE)\n",
    "RE_LEFT_PARENTH_FILTER = re.compile(r'[\\(\\[\\{\\⁽\\₍\\❨\\❪\\﹙\\（]', re.UNICODE)\n",
    "RE_RIGHT_PARENTH_FILTER = re.compile(r'[\\)\\]\\}\\⁾\\₎\\❩\\❫\\﹚\\）]', re.UNICODE)\n",
    "RE_BASIC_CLEANER = re.compile(r'[^\\w\\s\\-\\)\\(]', re.UNICODE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and cleaning data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# strong cleaner, replace any characters not in CHARS with a space\n",
    "def clean_text(txt):\n",
    "    txt = RE_DASH_FILTER.sub(' ', txt)\n",
    "    txt = RE_APOSTROPHE_FILTER.sub(' ', txt)\n",
    "    txt = RE_LEFT_PARENTH_FILTER.sub(' ', txt)\n",
    "    txt = RE_RIGHT_PARENTH_FILTER.sub(' ', txt)\n",
    "    txt = RE_BASIC_CLEANER.sub(' ', txt)\n",
    "    txt = \"\".join(char for char in txt if char in CHARS)\n",
    "    txt = NORMALIZE_WHITESPACE_REGEX.sub(' ', txt)\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading pre-cleaned file\n",
      "total: 2466169 lines\n",
      "max length: 9803 char\n",
      "CPU times: user 2.13 s, sys: 247 ms, total: 2.38 s\n",
      "Wall time: 2.39 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "i = 0\n",
    "length = 0\n",
    "lines = list()\n",
    "\n",
    "if os.path.isfile(CLEAN_FILEPATH):\n",
    "    print(\"loading pre-cleaned file\")\n",
    "    with open(CLEAN_FILEPATH, 'r', encoding='utf-8') as f:\n",
    "        for s in f:\n",
    "            line = s.rstrip('\\n')\n",
    "            lines.append(line)\n",
    "            i += 1\n",
    "            new_len = len(s)\n",
    "            if new_len > length:\n",
    "                length = new_len\n",
    "else:\n",
    "    print(\"loading and cleaning text\")\n",
    "    with open(FILEPATH, 'r', encoding='utf-8') as f:\n",
    "        for s in tqdm_notebook(f, total=N_LINES):\n",
    "            line = clean_text(s.lower()).strip('\\n')\n",
    "            if line != '':\n",
    "                lines.append(line)\n",
    "                i += 1\n",
    "                new_len = len(s)\n",
    "                if new_len > length:\n",
    "                    length = new_len\n",
    "\n",
    "    with open(CLEAN_FILEPATH, 'w', encoding='utf-8') as f:\n",
    "        for line in lines:\n",
    "            f.write(line + '\\n')\n",
    "\n",
    "n_lines = i\n",
    "print(\"total: %d lines\" % n_lines)\n",
    "print(\"max length: %d char\" % length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating mispelled sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_noise_to_string(a_string, amount_of_noise):\n",
    "    \n",
    "    length = len(a_string)\n",
    "    threshold = amount_of_noise * length\n",
    "    \n",
    "    # replace a character with a random character\n",
    "    if rand() < threshold:\n",
    "        rdm_char_pos = randint(length)\n",
    "        a_string = a_string[:rdm_char_pos] + choice(CHARS[:-1]) + a_string[rdm_char_pos + 1:]\n",
    "        \n",
    "    # delete a character\n",
    "    if rand() < threshold:\n",
    "        rdm_char_pos = randint(length)\n",
    "        a_string = a_string[:rdm_char_pos] + a_string[rdm_char_pos + 1:]\n",
    "        \n",
    "    # add a random character\n",
    "    if length < MAX_INPUT_LEN and rand() < threshold:\n",
    "        rdm_char_pos = randint(length)\n",
    "        a_string = a_string[:rdm_char_pos] + choice(CHARS[:-1]) + a_string[rdm_char_pos:]\n",
    "        \n",
    "    # transpose 2 characters\n",
    "    if rand() < threshold:\n",
    "        rdm_char_pos = randint(length - 2)\n",
    "        a_string = (a_string[:rdm_char_pos] +\n",
    "                    a_string[rdm_char_pos + 1] +\n",
    "                    a_string[rdm_char_pos] +\n",
    "                    a_string[rdm_char_pos + 2:])\n",
    "        \n",
    "    # delete space\n",
    "    if rand() < threshold:\n",
    "        spaces_pos = [pos for pos, char in enumerate(a_string) if char == \" \"]\n",
    "        if len(spaces_pos) != 0:\n",
    "            rdm_space_pos = choice(spaces_pos)\n",
    "            a_string = a_string[:rdm_space_pos] + a_string[rdm_space_pos + 1:]\n",
    "        \n",
    "    return a_string\n",
    "\n",
    "\n",
    "def generate_examples(corpus):\n",
    "    \n",
    "    sources, targets = list(), list()\n",
    "    \n",
    "    while corpus:\n",
    "        line = corpus.pop()\n",
    "        \n",
    "        while len(line) > MIN_INPUT_LEN:\n",
    "            if len(line) <= MAX_INPUT_LEN:\n",
    "                target = line\n",
    "                line = \"\"\n",
    "            else:\n",
    "                space_pos = line.rfind(\" \", MIN_INPUT_LEN, MAX_INPUT_LEN - 1)\n",
    "                if space_pos > -1:\n",
    "                    target = line[:space_pos]\n",
    "                    line = line[space_pos + 1:]\n",
    "                else:\n",
    "                    space_pos = line.rfind(\" \")\n",
    "                    if space_pos == -1:\n",
    "                        break\n",
    "                    else:\n",
    "                        line = line[space_pos + 1:]\n",
    "                        continue\n",
    "            targets.append(target)\n",
    "    \n",
    "    for target_idx, target in enumerate(targets):\n",
    "        source = add_noise_to_string(target, AMOUNT_OF_NOISE)\n",
    "        source += \".\" * (MAX_INPUT_LEN - len(source))\n",
    "        target += \".\" * (MAX_INPUT_LEN - len(target))\n",
    "        targets[target_idx] = target\n",
    "        sources.append(source)\n",
    "        \n",
    "    return sources, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 50s, sys: 881 ms, total: 1min 51s\n",
      "Wall time: 1min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "corpus = lines.copy()\n",
    "sources, targets = generate_examples(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to indian prime ministermanmohan........\n",
      "to indian prime minister manmohan.......\n"
     ]
    }
   ],
   "source": [
    "rd_idx = randint(0, len(sources))\n",
    "print(sources[rd_idx])\n",
    "print(targets[rd_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocabularies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PAD, GO, EOS, UNK = SEQ_VOC = ['.', ' _GO ', ' _EOS ', ' _UNK ']\n",
    "VOC = SEQ_VOC + CHARS\n",
    "\n",
    "vocabulary = dict()\n",
    "for i, token in enumerate(VOC):\n",
    "    vocabulary[token] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "president rene preval and former us.....\n",
      "president rene preval and former us.....\n"
     ]
    }
   ],
   "source": [
    "for _i, (_src, _tgt) in enumerate(zip(sources, targets)):\n",
    "    print(_i)\n",
    "    print(_src)\n",
    "    print(_tgt)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert character sequences into id sequences\n",
    "def char2id(char_seq):\n",
    "    return [vocabulary[char] for char in char_seq]\n",
    "\n",
    "def tokenize(src_strings, tgt_strings, reverse=True):\n",
    "    n = len(src_strings)\n",
    "    d = MAX_INPUT_LEN\n",
    "    \n",
    "    empty_ids = np.empty(shape=(n, d*2+1), dtype=np.int32)\n",
    "    empty_ids.fill(vocabulary[PAD])\n",
    "    src_ids = np.copy(empty_ids)\n",
    "    tgt_ids = np.copy(empty_ids)\n",
    "    \n",
    "    gen = enumerate(zip(src_strings, tgt_strings))\n",
    "    for i, (src_seq, tgt_seq) in tqdm_notebook(gen, total=n):\n",
    "        src_seq = char2id(src_seq)\n",
    "        tgt_seq = char2id(tgt_seq)\n",
    "        src_ids[i] = src_seq[::-1] + [vocabulary[GO]] + tgt_seq\n",
    "        tgt_ids[i, d:-1] = tgt_seq\n",
    "        t = 1\n",
    "        while not tgt_ids[i, t]:\n",
    "            t -= 1\n",
    "        tgt_ids[i, t+1] = vocabulary[EOS]\n",
    "        \n",
    "    return src_ids, tgt_ids\n",
    "\n",
    "# convert id sequences into character sequences\n",
    "def id2char(id_seq):\n",
    "    return \"\".join(VOC[idx] for idx in id_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e85952514a8c472eb56461b26d8814ae"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 2min 54s, sys: 1.58 s, total: 2min 56s\n",
      "Wall time: 2min 56s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "x, y = tokenize(sources, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  0,  0,  0,  0, 22, 19, 24, 15, 13, 26,  4, 14,  9, 24, 23, 13,\n",
       "       18, 13, 17,  4,  9, 17, 13, 22, 20,  4, 23,  4, 29, 22, 24, 18, 25,\n",
       "       19,  7,  4,  9, 12, 24,  1, 24, 12,  9,  4,  7, 19, 25, 18, 24, 22,\n",
       "       29,  4, 23,  4, 20, 22, 13, 17,  9,  4, 17, 13, 18, 13, 23, 24,  9,\n",
       "       22,  4, 26, 13, 15, 24, 19, 22,  0,  0,  0,  0,  0], dtype=int32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0, 24, 12,  9,  4,  7, 19, 25, 18, 24, 22, 29,\n",
       "        4, 23,  4, 20, 22, 13, 17,  9,  4, 17, 13, 18, 13, 23, 24,  9, 22,\n",
       "        4, 26, 13, 15, 24, 19, 22,  2,  0,  0,  0,  0,  0], dtype=int32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'........................................the country s prime minister viktor _EOS .....'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2char(y[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.....rotkiv jetsinim emirp s yrtnuoc eht _GO the country s prime minister viktor.....'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2char(x[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "n_try = 100000\n",
    "x_train, x_test, y_train, y_test = train_test_split(x[:n_try], y[:n_try], train_size=0.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seq2Seq architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Dropout, GRU, Dense\n",
    "\n",
    "length = np.shape(x)[1]\n",
    "voc_size = len(VOC)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(voc_size, 64, input_length=length))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(GRU(512, return_sequences=True))\n",
    "model.add(Dense(voc_size, activation='softmax'))\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "\n",
    "best_model_fname = \"./spellcheck_seq2seq_chekpoint.h5\"\n",
    "best_model_cb = ModelCheckpoint(best_model_fname,\n",
    "                                monitor='val_loss',\n",
    "                                save_best_only=True, \n",
    "                                verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 75000 samples, validate on 25000 samples\n",
      "Epoch 1/3\n",
      "74944/75000 [============================>.] - ETA: 0s - loss: 1.0425Epoch 00000: val_loss improved from inf to 0.78015, saving model to ./spellcheck_seq2seq_chekpoint.h5\n",
      "75000/75000 [==============================] - 626s - loss: 1.0424 - val_loss: 0.7801\n",
      "Epoch 2/3\n",
      "74944/75000 [============================>.] - ETA: 0s - loss: 0.6575Epoch 00001: val_loss improved from 0.78015 to 0.48141, saving model to ./spellcheck_seq2seq_chekpoint.h5\n",
      "75000/75000 [==============================] - 629s - loss: 0.6574 - val_loss: 0.4814\n",
      "Epoch 3/3\n",
      "74944/75000 [============================>.] - ETA: 0s - loss: 0.4071Epoch 00002: val_loss improved from 0.48141 to 0.27378, saving model to ./spellcheck_seq2seq_chekpoint.h5\n",
      "75000/75000 [==============================] - 626s - loss: 0.4070 - val_loss: 0.2738\n",
      "CPU times: user 3h 23min 12s, sys: 54min 20s, total: 4h 17min 33s\n",
      "Wall time: 31min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "validation_data = (x_test, np.expand_dims(y_test, -1))\n",
    "history = model.fit(x_train, \n",
    "                    np.expand_dims(y_train, -1), \n",
    "                    validation_data=validation_data, \n",
    "                    epochs=3, \n",
    "                    batch_size=64, \n",
    "                    callbacks=[best_model_cb]\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4,  9, 17, 13,  4, 24,  4,  9, 12, 24,  4, 24,  5,  4, 22,  9, 24,\n",
       "       23, 13, 18, 13, 17,  4,  9, 17, 13, 22, 20,  4, 23,  5, 27,  4, 13,\n",
       "       16,  5, 17, 18,  9,  6,  1,  6,  9, 18,  4,  5, 16, 13,  4, 27,  5,\n",
       "       23,  4, 20, 22, 13, 17,  9,  4, 17, 13, 18, 13, 23, 24,  9, 22,  4,\n",
       "        5, 24,  4, 24, 12,  9,  4, 24, 13, 17,  9,  4,  0], dtype=int32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' emi t eht ta retsinim emirp saw ilamneb _GO ben ali was prime minister at the time .'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2char(x_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.predict([x_test[0]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
