{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=firebrick> Tmp header for reloading </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "from numpy.random import rand, randint, choice, shuffle\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# parameters\n",
    "FILEPATH = \"./data/news.2011.en.shuffled\"\n",
    "CLEAN_FILEPATH = \"./data/news.2011.en.cleaned\"\n",
    "N_LINES = 2466169\n",
    "CHARS = list(\" abcdefghijklmnopqrstuvwxyz\")\n",
    "MAX_INPUT_LEN = 40\n",
    "MIN_INPUT_LEN = 3\n",
    "AMOUNT_OF_NOISE = 0.3 / MAX_INPUT_LEN\n",
    "\n",
    "# regex cleanup\n",
    "RE_DASH_FILTER = re.compile(r'[\\-\\˗\\֊\\‐\\‑\\‒\\–\\—\\⁻\\₋\\−\\﹣\\－]', re.UNICODE)\n",
    "NORMALIZE_WHITESPACE_REGEX = re.compile(r'[^\\S\\n]+', re.UNICODE)\n",
    "RE_APOSTROPHE_FILTER = re.compile(r'&#39;|[ʼ՚＇‘’‛❛❜ߴߵ`‵´ˊˋ{}{}{}{}{}{}{}{}{}]'.\n",
    "                                  format ( \n",
    "                                      chr(768), chr(769), chr(832), \n",
    "                                      chr(833), chr(2387), chr(5151), \n",
    "                                      chr(5152), chr(65344), chr(8242)\n",
    "                                  ), re.UNICODE)\n",
    "RE_LEFT_PARENTH_FILTER = re.compile(r'[\\(\\[\\{\\⁽\\₍\\❨\\❪\\﹙\\（]', re.UNICODE)\n",
    "RE_RIGHT_PARENTH_FILTER = re.compile(r'[\\)\\]\\}\\⁾\\₎\\❩\\❫\\﹚\\）]', re.UNICODE)\n",
    "RE_BASIC_CLEANER = re.compile(r'[^\\w\\s\\-\\)\\(]', re.UNICODE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and cleaning data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# strong cleaner, replace any characters not in CHARS with a space\n",
    "def clean_text(txt):\n",
    "    txt = RE_DASH_FILTER.sub(' ', txt)\n",
    "    txt = RE_APOSTROPHE_FILTER.sub(' ', txt)\n",
    "    txt = RE_LEFT_PARENTH_FILTER.sub(' ', txt)\n",
    "    txt = RE_RIGHT_PARENTH_FILTER.sub(' ', txt)\n",
    "    txt = RE_BASIC_CLEANER.sub(' ', txt)\n",
    "    txt = \"\".join(char for char in txt if char in CHARS)\n",
    "    txt = NORMALIZE_WHITESPACE_REGEX.sub(' ', txt)\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading pre-cleaned file\n",
      "total: 2466169 lines\n",
      "max length: 9803 char\n",
      "CPU times: user 2.14 s, sys: 232 ms, total: 2.37 s\n",
      "Wall time: 2.36 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "i = 0\n",
    "length = 0\n",
    "lines = list()\n",
    "\n",
    "if os.path.isfile(CLEAN_FILEPATH):\n",
    "    print(\"loading pre-cleaned file\")\n",
    "    with open(CLEAN_FILEPATH, 'r', encoding='utf-8') as f:\n",
    "        for s in f:\n",
    "            line = s.rstrip('\\n')\n",
    "            lines.append(line)\n",
    "            i += 1\n",
    "            new_len = len(s)\n",
    "            if new_len > length:\n",
    "                length = new_len\n",
    "else:\n",
    "    print(\"loading and cleaning text\")\n",
    "    with open(FILEPATH, 'r', encoding='utf-8') as f:\n",
    "        for s in tqdm_notebook(f, total=N_LINES):\n",
    "            line = clean_text(s.lower()).strip('\\n')\n",
    "            if line != '':\n",
    "                lines.append(line)\n",
    "                i += 1\n",
    "                new_len = len(s)\n",
    "                if new_len > length:\n",
    "                    length = new_len\n",
    "\n",
    "    with open(CLEAN_FILEPATH, 'w', encoding='utf-8') as f:\n",
    "        for line in lines:\n",
    "            f.write(line + '\\n')\n",
    "\n",
    "n_lines = i\n",
    "print(\"total: %d lines\" % n_lines)\n",
    "print(\"max length: %d char\" % length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating mispelled sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_noise_to_string(a_string, amount_of_noise):\n",
    "    \n",
    "    length = len(a_string)\n",
    "    threshold = amount_of_noise * length\n",
    "    \n",
    "    # replace a character with a random character\n",
    "    if rand() < threshold:\n",
    "        rdm_char_pos = randint(length)\n",
    "        a_string = a_string[:rdm_char_pos] + choice(CHARS[:-1]) + a_string[rdm_char_pos + 1:]\n",
    "        \n",
    "    # delete a character\n",
    "    if rand() < threshold:\n",
    "        rdm_char_pos = randint(length)\n",
    "        a_string = a_string[:rdm_char_pos] + a_string[rdm_char_pos + 1:]\n",
    "        \n",
    "    # add a random character\n",
    "    if length < MAX_INPUT_LEN and rand() < threshold:\n",
    "        rdm_char_pos = randint(length)\n",
    "        a_string = a_string[:rdm_char_pos] + choice(CHARS[:-1]) + a_string[rdm_char_pos:]\n",
    "        \n",
    "    # transpose 2 characters\n",
    "    if rand() < threshold:\n",
    "        rdm_char_pos = randint(length - 2)\n",
    "        a_string = (a_string[:rdm_char_pos] +\n",
    "                    a_string[rdm_char_pos + 1] +\n",
    "                    a_string[rdm_char_pos] +\n",
    "                    a_string[rdm_char_pos + 2:])\n",
    "        \n",
    "    # delete space\n",
    "    if rand() < threshold:\n",
    "        spaces_pos = [pos for pos, char in enumerate(a_string) if char == \" \"]\n",
    "        if len(spaces_pos) != 0:\n",
    "            rdm_space_pos = choice(spaces_pos)\n",
    "            a_string = a_string[:rdm_space_pos] + a_string[rdm_space_pos + 1:]\n",
    "        \n",
    "    return a_string\n",
    "\n",
    "\n",
    "def generate_examples(corpus):\n",
    "    \n",
    "    sources, targets = list(), list()\n",
    "    \n",
    "    print(\"split sentences...\")\n",
    "    while corpus:\n",
    "        line = corpus.pop()\n",
    "        \n",
    "        while len(line) > MIN_INPUT_LEN:\n",
    "            if len(line) <= MAX_INPUT_LEN:\n",
    "                target = line\n",
    "                line = \"\"\n",
    "            else:\n",
    "                space_pos = line.rfind(\" \", MIN_INPUT_LEN, MAX_INPUT_LEN - 1)\n",
    "                if space_pos > -1:\n",
    "                    target = line[:space_pos]\n",
    "                    line = line[space_pos + 1:]\n",
    "                else:\n",
    "                    space_pos = line.rfind(\" \")\n",
    "                    if space_pos == -1:\n",
    "                        break\n",
    "                    else:\n",
    "                        line = line[space_pos + 1:]\n",
    "                        continue\n",
    "            targets.append(target)\n",
    "    print(\"...done\")\n",
    "    \n",
    "    for target_idx, target in tqdm_notebook(enumerate(targets), total=len(targets)):\n",
    "        source = add_noise_to_string(target, AMOUNT_OF_NOISE)\n",
    "        source += \".\" * (MAX_INPUT_LEN - len(source))\n",
    "        target += \".\" * (MAX_INPUT_LEN - len(target))\n",
    "        targets[target_idx] = target\n",
    "        sources.append(source)\n",
    "        \n",
    "    return sources, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split sentences...\n",
      "...done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed or enabled properly.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45e89cda30bb4439b2cdc25720ae8eff"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 2min 3s, sys: 1.24 s, total: 2min 4s\n",
      "Wall time: 2min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "corpus = lines.copy()\n",
    "sources, targets = generate_examples(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thtey have qor a long time yet the......\n",
      "they have for a long time yet the.......\n"
     ]
    }
   ],
   "source": [
    "rd_idx = randint(0, len(sources))\n",
    "print(sources[rd_idx])\n",
    "print(targets[rd_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocabularies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PAD, GO, EOS, UNK = SEQ_VOC = ['.', ' _GO ', ' _EOS ', ' _UNK ']\n",
    "VOC = SEQ_VOC + CHARS\n",
    "\n",
    "vocabulary = dict()\n",
    "for i, token in enumerate(VOC):\n",
    "    vocabulary[token] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert character sequences into id sequences\n",
    "def char2id(char_seq):\n",
    "    return [vocabulary[char] for char in char_seq]\n",
    "    \n",
    "def tokenize(src_strings, tgt_strings):\n",
    "    n = len(src_strings)\n",
    "    d = MAX_INPUT_LEN\n",
    "    \n",
    "    empty_ids = np.zeros(shape=(n, d*2+1), dtype=np.int32)\n",
    "    src_ids = np.zeros(shape=(n, d*2+1), dtype=np.int32)\n",
    "    tgt_ids = np.zeros(shape=(n, d*2+1), dtype=np.int32)\n",
    "    \n",
    "    for i, (src_seq, tgt_seq) in tqdm_notebook(enumerate(zip(src_strings, tgt_strings)), total=n):\n",
    "        src_seq = char2id(src_seq)\n",
    "        tgt_seq = char2id(tgt_seq)\n",
    "        src_ids[i] = src_seq[::-1] + [vocabulary[GO]] + tgt_seq\n",
    "        tgt_ids[i, d:-1] = tgt_seq\n",
    "        t = 1\n",
    "        while not tgt_ids[i, t]:\n",
    "            t -= 1\n",
    "        tgt_ids[i, t+1] = vocabulary[EOS]\n",
    "        \n",
    "    return src_ids, tgt_ids\n",
    "\n",
    "# convert id sequences into character sequences\n",
    "def id2char(id_seq):\n",
    "    return \"\".join(VOC[idx] for idx in id_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed or enabled properly.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8dcd13c179840d786c3c9554e3c6eee"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 4min 24s, sys: 51.5 s, total: 5min 16s\n",
      "Wall time: 5min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "x, y = tokenize(sources, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 24 12  9  4 25 23  4 20 22  9\n",
      " 23 13  8  9 18 24 13  5 16  4  7 19 17 17 13 23 23 13 19 18  4 23  2  0  0\n",
      "  0  0  0  0  0  0]\n",
      "\n",
      "[ 0  0  0  0  0  0  0  0  0 23  4 18 19 13 23 23 13 17 19  7  4 16  5 13 24\n",
      " 18  9  8 13 23  9 22 20  4 23 25  4  9 12 24  1 24 12  9  4 25 23  4 20 22\n",
      "  9 23 13  8  9 18 24 13  5 16  4  7 19 17 17 13 23 23 13 19 18  4 23  0  0\n",
      "  0  0  0  0  0  0]\n",
      "\n",
      "........................................the us presidential commission s _EOS ........\n",
      ".........s noissimoc laitnediserp su eht _GO the us presidential commission s........\n"
     ]
    }
   ],
   "source": [
    "idx = 15\n",
    "print(y[idx])\n",
    "print()\n",
    "print(x[idx])\n",
    "print()\n",
    "print(id2char(y[idx]))\n",
    "print(id2char(x[idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seq2Seq architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "n_try = 100000\n",
    "x_train, x_test, y_train, y_test = train_test_split(x[:n_try], y[:n_try], train_size=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Dropout, GRU, Dense\n",
    "\n",
    "length = np.shape(x)[1]\n",
    "voc_size = len(VOC)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(voc_size, 64, input_length=length))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(GRU(1024, return_sequences=True))\n",
    "model.add(GRU(1024, return_sequences=True))\n",
    "model.add(Dense(voc_size, activation='softmax'))\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "\n",
    "best_model_fname = \"./seq2seq_model_checkpoint.h5\"\n",
    "best_model_cb = ModelCheckpoint(best_model_fname,\n",
    "                                monitor='val_loss',\n",
    "                                save_best_only=True, \n",
    "                                verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 75000 samples, validate on 25000 samples\n",
      "Epoch 1/5\n",
      "  896/75000 [..............................] - ETA: 3254s - loss: 2.4636"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "validation_data = (x_test, np.expand_dims(y_test, -1))\n",
    "history = model.fit(x_train, \n",
    "                    np.expand_dims(y_train, -1), \n",
    "                    validation_data=validation_data, \n",
    "                    epochs=5,\n",
    "                    batch_size=64, \n",
    "                    verbose=1,\n",
    "                    callbacks=[best_model_cb]\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def greedy_correct(model, input_ids):\n",
    "    \n",
    "    if type(input_ids) == np.ndarray:\n",
    "        input_ids = input_ids.tolist()\n",
    "    assert len(input_ids) == 40\n",
    "    \n",
    "    input_ids += [vocabulary[GO]]\n",
    "    input_array = np.empty(shape=(1, model.input_shape[1]), dtype=np.int32)\n",
    "    decoded_ids = []\n",
    "    \n",
    "    while len(input_ids) <= length:\n",
    "        \n",
    "        input_array.fill(vocabulary[PAD])\n",
    "        input_array[0, -len(input_ids):] = input_ids\n",
    "        \n",
    "        next_token_id = model.predict(input_array)[0, -1].argmax()\n",
    "        \n",
    "        if next_token_id == vocabulary[EOS]:\n",
    "            break\n",
    "            \n",
    "        decoded_ids.append(next_token_id)\n",
    "        \n",
    "        input_ids.append(next_token_id)\n",
    "    \n",
    "    return id2char(decoded_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "# best_model = load_model(\"./seq2seq_model_checkpoint.h5\")\n",
    "# best_model = load_model(\"./seq2seq_model_checkpoint_1024_008.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_test = np.shape(x_test)[0]\n",
    "idx = np.random.randint(n_test)\n",
    "print(\"idx: %s\" % idx)\n",
    "print()\n",
    "input_ids = x_test[idx][:40]\n",
    "expected_ids = x_test[idx][41:]\n",
    "\n",
    "print(\"mispelled string:\")\n",
    "print(id2char(input_ids)[::-1])\n",
    "print()\n",
    "print(\"deep corrected string:\")\n",
    "corrected_string = greedy_correct(best_model, input_ids)\n",
    "print(corrected_string)\n",
    "print()\n",
    "print(\"expected correction:\")\n",
    "print(id2char(expected_ids))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
